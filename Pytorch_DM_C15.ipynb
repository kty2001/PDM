{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfWgAXc9zXfCFe+VP8hpcf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 15장 - 제품으로 배포하기"],"metadata":{"id":"HRN_Dfd-JvSF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbGPEsF0JqYz"},"outputs":[],"source":["# 딥러닝 모델 추론을 대규모로 수행하도록 인프라 구조를 관리하는 것은 비용이나 아키텍처 측면에서 인상적임\n","# 파이토치는 다양한 양산용 기능이 추가되면서 대규모 제품화까지 지원하며 전 분야를 아우르는 엔드투엔드 플랫폼으로 발전함"]},{"cell_type":"code","source":["# 제품으로 배포한다는 이야기는 사용 사례에 따라 다음과 같이 해석 가능함\n","# 1. 모델에 접근 가능한 네트워크 서비스 설정하기: 플라스크(Flask)와 새니(Sanic) 두가지 경량 파이썬 웹 프레임워크 사용\n","# 2. 모델을 표준화된 포맷으로 내보내어 최적화된 모델 프로세서나 특수 하드웨어, 클라우드 서비스에 출시할 수 있도록 준비. 파이토치 모델은 이를 위해 ONNX(open neural network exchange) 사용\n","# 3. 더 큰 애플리케이션의 일부로 통합하고 싶을 경우: 모델이 파이썬으로 제한되지 않아야 좋으므로 다른 언어로의 징검다리 역할하는 C++파이토치 모델 사용\n","# 4. 모바일 기기에서 동작: 최근 파이토치는 모바일 지원하기 시작함"],"metadata":{"id":"fO3WLQbBWB4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 14장의 분류기 사용해 서빙한 다음 얼룩말 모델 사용해 배포 방식 설명"],"metadata":{"id":"OlwQQ1dxW17m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1절 - 파이토치 모델 서빙"],"metadata":{"id":"iZE5RnYBXUVy"}},{"cell_type":"code","source":["# 모델 서버에 올리는 것부터 시작\n","# 서버 만들고 단순 동작을 확인하고 단점 확인한 후 개선\n","# 최종 완성물 살펴보고 네트워크 요청을 대기하도록 제작"],"metadata":{"id":"vz7UNOPXXUAU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .1 플라스크에 들어간 모델"],"metadata":{"id":"4uL2hZHzXlYj"}},{"cell_type":"code","source":["# 플라스크는 파이썬 모듈 중 가장 많이 사용되며 pip로 설치 가능\n","# API는 데코레이터 사용하여 만듦\n","# flask_hello_world.py\n","from flask import Flask\n","app = Flask(__name__)\n","\n","@app.route(\"/hello\")\n","def hello():\n","    return \"Hello World!\"\n","\n","if __name__ == '__main__':\n","    app.run(host='0.0.0.0', port=8000)"],"metadata":{"id":"A5VCSbD8XoYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행하면 애플리케이션은 8000번 포트 듣고 있으며, /hello 경로로 들어오는 요청에 대해 Hello World 문자열 리턴하도록 동작\n","# 이 플라스크 서버에 앞서 저장했던 모델을 로딩하여 POST 경로로 연결 가능"],"metadata":{"id":"jR6qJ05kX9gS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터는 플라스크의 request를 통해 데이터 입력받음\n","# 정확히는 request.files는 필드 이름으로 접근할 수 있는 파일 객체의 딕셔너리 포함\n","# 입력은 JSON으로 파싱하고 플라스크의 jsonify 헬퍼를 사용하여 JSON 문자열로 반환"],"metadata":{"id":"NydtaOh-YQOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# /hello 대신 /predict 경로 열어 바이너리 블록(시리즈 데이터의 픽셀 내용)과 연관된 메타데이터(키가 shape인 딕셔너리 포함하는 JSON 객체)를 POST 요청으로 제공된 입력 파일로 받고 예측한 결과를 JSON응답으로 리턴\n","# 데이터를 얻기 위해 가장 먼저 해야 할 작업은 JSON을 디코드해 바이너리로 바꾸는 것이고, 바꾼 바이너리는 numpy.frombuffer를 사용해 1차원 배열로 디코딩\n","# 이 배열을 torch.from_numpy를 사용해 텐서와 실제 차원 정보에 맞는 뷰로 변환\n","# 모델링은 LunaModel을 인스턴스로 만들고 훈련에서 얻은 가중치를 로딩한 후 모델을 eval 모드로 설정\n","# 훈련 필요 없으므로 with torch.no_grad()블록에서 모델 돌릴 때 기울기 필요없다고 알려줌"],"metadata":{"id":"1IZZ15oBa51M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# flask_server.py\n","import numpy as np\n","import sys\n","import os\n","import torch\n","from flask import Flask, request, jsonify\n","import json\n","\n","from p2ch13.model_cls import LunaModel\n","\n","app = Flask(__name__)\n","\n","model = LunaModel() # 모델 설정하고 가중치 읽어들 후 평가 모드로 전환\n","model.load_state_dict(torch.load(sys.argv[1],\n","                                 map_location='cpu')['model_state'])\n","model.eval()\n","\n","def run_inference(in_tensor):\n","    with torch.no_grad(): # 자동미분 없음\n","        # LunaModel takes a batch and outputs a tuple (scores, probs)\n","        out_tensor = model(in_tensor.unsqueeze(0))[1].squeeze(0)\n","    probs = out_tensor.tolist()\n","    out = {'prob_malignant': probs[1]}\n","    return out\n","\n","@app.route(\"/predict\", methods=[\"POST\"]) # /predict 엔드포인트에서 폼 입력(HTTP_POST) 받음\n","def predict():\n","    meta = json.load(request.files['meta']) # request는 meta로 부르는 파일 하나 가짐\n","    blob = request.files['blob'].read()\n","    in_tensor = torch.from_numpy(np.frombuffer(\n","        blob, dtype=np.float32)) # 바이너리 블롭을 토치로 변환\n","    in_tensor = in_tensor.view(*meta['shape'])\n","    out = run_inference(in_tensor)\n","    return jsonify(out) # 응답 콘텐츠를 JSON으로 인코딩\n","\n","if __name__ == '__main__':\n","    app.run(host='0.0.0.0', port=8000)\n","    print (sys.argv[1])"],"metadata":{"id":"4vHpIBcucQ99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 서버 띄우기\n","python3 -m p3ch15.flask_server data/part2/models/cls_2019-10-19_15.48.24_final_cls.best.state"],"metadata":{"id":"IcjBvWJhc4qT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 샘플 하나 보내는 간단한 클라이언트 cls_client.py 구현함\n","# 디렉토리에서 실행\n","python p3ch15/cls_client.py"],"metadata":{"id":"VtKxKdS0dJj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행 시 해당 결절은 악성 아닐 가능성이 높다고 출력됨\n","# 서버가 입력을 받아 모델을 실행하여 결과를 출력"],"metadata":{"id":"HwMmbnOIdT_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .2 배포 시 바라는 점"],"metadata":{"id":"XYrCWPCSdbX5"}},{"cell_type":"code","source":["# 모델 서빙 시 고려할 점\n","# 최신 프로토콜과 기능 지원해야 함\n","# 부분적인 개선 만들어내기 위해 새닉으로 프레임워크를 업그레이드하면 효율적으로 계산 가능\n","# 특히 GPU 사용하는 경우 배치로 여러 요청을 모아 처리하는 것이 효율적임\n","# 여러 커넥션에서 복수의 요청을 모은 작업을 배치로 묶어 GPU에서 실행한 후 결과를 각 요청에 리턴\n","# 하나의 GPU에 여러 개의 배치 돌릴 이유는 없으므로 배치 크기를 최대로 올리는 것이 일반적으로 더 효율적임"],"metadata":{"id":"9VBCDs6Ldm2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 여러 작업을 병렬로 수행하길 원함\n","# 비동기 서빙의 두 번째 스레드에서 동작하는 모델도 효율적으로 동작하게 만들길 원함\n","# 즉 파이썬 GIL로부터 모델이 벗어나게 만들고 싶음"],"metadata":{"id":"Lc2oxXmohYwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 메모리 복사도 최소로 줄이고 싶음\n","# 메모리 사용량과 시간 측면에서 뭔가를 계속 복사하는 방식은 나쁨\n","# HTTP에서는 Base64 인코딩을 많이 사용함\n","# 부분적인 개선을 만들기 위해 스트리밍 PUT 요청으로 Base64 문자열을 할당하지 않도록 만들고 문자열을 뒤에 덧붙여 가며 늘리는 일 피하기"],"metadata":{"id":"r4gGxwXfhyCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 안전성 문제\n","# 자원 고갈이나 오버플로 방지를 위해 안전한 디코딩 방식 원함\n","# 고정된 크기의 입력으로 시작한 파이토치는 충돌 일으키기 어렵지만 이를 위해 이미지 디코딩하고 크기 조정하는 식의 방법은 골 아프고 보장하기 어려움\n","# 책에서는 패스"],"metadata":{"id":"iS_BajOBiKKv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .3 배치 요청"],"metadata":{"id":"Qtn4xaoAimUz"}},{"cell_type":"code","source":["# 두 번째 서버는 새닉 프레임워크를 사용\n","# 새닉을 이용하면 비동기 처리 방식으로 여러 요청을 병렬로 서빙 가능\n","# 요청을 배치로 만드는 것도 구현"],"metadata":{"id":"QiDS7IDdil4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 배치 요청 처리하려면 요청 처리 과정에서 모델 실행 분리해야 함"],"metadata":{"id":"RuYNz2mIi3Vd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구현\n","# 두 개의 함수로 구현하기"],"metadata":{"id":"9LBQEAIyjJEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 실행 함수는 한번 시작하면 영원히 동작\n","# 모델을 실행할 필요 있을 때마다, 모델 실행 함수는 입력을 배치로 구성하고 두 번째 스레드에서 모델을 실행한 다음 결과 반환"],"metadata":{"id":"f_IPbRQQjXzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 요청 프로세서는 요청을 디코딩하고 입력을 큐에 넣은 후 처리가 끝나기를 기다린 다음, 출력 결과를 담아 반환\n","# 새 요청이 들어오면 큐에 넣고 필요하면 처리하도록 시그널 준 후 요청에 대한 응답으로 보내기 전에 결과를 기다림\n","# 배치가 요청으로 꽉 차거나 제일 먼저 들어온 요청의 대기 시간이 타이머를 넘으면 이벤트 처리 시작"],"metadata":{"id":"h5HUfQvvjcwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 대부분의 주요 코드는 ModelRunner 클래스에서 확인 가능\n","# request_batching_server.py:32\n","class ModelRunner:\n","    def __init__(self, model_name):\n","        self.model_name = model_name\n","        self.queue = []\n","\n","        self.queue_lock = None # lock으로 동작\n","\n","        self.model = get_pretrained_model(self.model_name,\n","                                          map_location=device) # 모델 읽어 인스턴스화 시킴. JIT으로 전환 시 이 부분만 수정하면 됨. 현재는 p3ch15/cyclegan.py에서 사이클 GAN 읽어들임\n","\n","        self.needs_processing = None # 모델 실행 시그널\n","\n","        self.needs_processing_timer = None # 타이머"],"metadata":{"id":"P0yhYBj8mgMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ModelRuuner는 먼저 모델 읽어들인 후 몇 가지 관리 작업 수행\n","# 모델 외에 다른 구성 요소도 필요"],"metadata":{"id":"m_PirFI8m7J0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# queue 수정 시 다른 작업이 큐 바꾸지 못하도록 asynio 모듈에서 제공하는 asynio.Lock인 queue_lock 사용\n","# 여기서 사용하는 모든 asynio 객체는 이벤트 루프 알아야 하며 이 루프는 애플리케이션 초기화가 끝난 후에 사용 가능\n","# 따라서 초기화 작업 때에는 임시로 None 설정\n","# 워커가 여러 개인 경우 락 지켜볼 필요 있음\n","# 주의사항: 파이썬의 비동기 락은 스레드 안전(threadsafe)하지 않음"],"metadata":{"id":"GRO-MZxZno7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 할 일 없는 경우 ModelRuuner는 대기\n","# 대기 마치고 일하는 시점을 시그널로 알리는 것은 RequestProcessor의 몫\n","# 실제로는 asynio.Event인 needs_processing을 통해 진행\n","# ModelRuuner는 needs_process 이벤트를 기다리기 위해 wait() 메소드 사용\n","# RequestProcessor는 시그널 주기 위해 set() 사용하며 ModelRunner가 깨어나면 clear()로 이벤트 시그널 리셋함"],"metadata":{"id":"cIKYm0jnoPB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최대 대기 시간 보앟기 위해 타이머 설정\n","# 필요할 때 app.loop.call_at으로 만들 수 있으며, 타이머는 needs_processing 이벤트를 예비로 하나 만들어 둠\n","# 타이머 다 되기 전 배치 처리하는 경우 만들어진 타이머용 이벤트 클리어하여 불필요한 호출 방지"],"metadata":{"id":"inoislgMonfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 요청부터 큐까지\n","# 다음으로 요청을 큐에 넣기\n","# 첫 번째 async 메소드인 process_input에서 수행\n","# request_batching_server.py:54\n","async def process_input(self, input):\n","    our_task = {\"done_event\": asyncio.Event(loop=app.loop), # task 데이터 설정\n","                \"input\": input,\n","                \"time\": app.loop.time()}\n","    async with self.queue_lock: # lock 건 후 태스크 추가\n","        if len(self.queue) >= MAX_QUEUE_SIZE:\n","            raise HandlingError(\"I'm too busy\", code=503)\n","        self.queue.append(our_task)\n","        logger.debug(\"enqueued task. new queue size {}\".format(len(self.queue)))\n","        self.schedule_processing_if_needed() # 스케줄링. 프로세싱 메소드는 배치가 다 찼으면 needs_processing 켜기. 배치가 다 차지 않았고 타이머 설정되지 않았다면 최대 대기 시간에 깨어 날수 있도록 타이머 켜기\n","\n","    await our_task[\"done_event\"].wait() # 처리 끝날 때까지 기다리기. 실행은 await로 루프에 넘기기\n","    return our_task[\"output\"]"],"metadata":{"id":"IDddnWbEpHem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 태스크 정보 보관을 위해 파이썬 딕셔너리를 사용\n","# 딕셔너리에는 input, 큐에 들어간 시간인 time, 작업 처리 후에 켜질 done_event 들어 있음\n","# 프로세싱은 여기에 output추가"],"metadata":{"id":"Y0iAiBQeqe1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 큐의 락을 유지하면서, 태스크를 큐에 넣고 필요에 따라 프로세싱을 스케줄링 함\n","# 큐 가득 차 있다면 에러 발생시킴\n","# 이후 태스크 끝나길 기다리고 반환만 하면 됨"],"metadata":{"id":"fm6-7QcVqtMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 큐에서 가져와 배치 실행하기\n","# request_batching_server.py:71\n","async def model_runner(self):\n","    self.queue_lock = asyncio.Lock(loop=app.loop)\n","    self.needs_processing = asyncio.Event(loop=app.loop)\n","    logger.info(\"started model runner for {}\".format(self.model_name))\n","    while True:\n","        await self.needs_processing.wait() # 처리할 작업 들어올 때까지 대기\n","        self.needs_processing.clear()\n","        if self.needs_processing_timer is not None: # 타이머 설정되어 있으면 클리어\n","            self.needs_processing_timer.cancel()\n","            self.needs_processing_timer = None\n","        async with self.queue_lock:\n","            if self.queue:\n","                longest_wait = app.loop.time() - self.queue[0][\"time\"]\n","            else:  # oops\n","                longest_wait = None\n","            logger.debug(\"launching processing. queue size: {}. longest wait: {}\".format(len(self.queue), longest_wait))\n","            to_process = self.queue[:MAX_BATCH_SIZE] # 배치 얻어, 필요한 경우 다음 배치 실행을 스케줄링\n","            del self.queue[:len(to_process)]\n","            self.schedule_processing_if_needed()\n","        # so here we copy, it would be neater to avoid this\n","        batch = torch.stack([t[\"input\"] for t in to_process], dim=0)\n","        # we could delete inputs here...\n","\n","        result = await app.loop.run_in_executor(\n","            None, functools.partial(self.run_model, batch) # 별도 스레드에서 모델 연산 수행하고 데이터를 디바이스로 옮겨 모델로 전달. 끝난 후 프로세싱의 나머지 수행\n","        )\n","        for t, r in zip(to_process, result): # 결과를 작업 아이템에 더하고 이를 알리는 이벤트 켜기\n","            t[\"output\"] = r\n","            t[\"done_event\"].set()\n","        del to_process"],"metadata":{"id":"nyQLkRcJrBXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model_ruuner는 설정 후 무한 루프 돌음\n","# 앱 시작할 때 호출되므로 queue_lock과 needs_processing 이벤트를 설정할 수 있음\n","# 이후 루프 안에서 needs_processing 이벤트를 await시킴"],"metadata":{"id":"_n1ukwFyrUDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이벤트 들어왔을 때 타이머가 설정됐다면 어차피 지금 프로세스 진행할 예정이므로 테이머 리셋\n","# model_runner는 큐에서 배치를 꺼내서 필요하면 다음번 배치 프로세싱을 스케줄링 함\n","# 이때 개별 태스크에서 배치를 만들고 모델 평가를 위한 스레드를 asynio의 app.loop.run_in_executor를 사용해 만듦\n","# 마지막으로 출력을 태스크에 더하고 done_event를 켬"],"metadata":{"id":"h1oajjLetNhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# asynio와 await가 뿌려진 플라스크 같은 웹 프레임워크는 래퍼가 필요\n","# model_runner 함수를 이벤트 루프에서 시작해야 함\n","# 여러 스레드가 큐에서 꺼내오면서 서로를 인터럽트하는 상황 아니면 큐에 락은 필요 없음\n","# 다른 프로젝트에 코드 적용할 때를 대비해 요청을 누락하는 가능성 없애도록 락을 사용"],"metadata":{"id":"JogXYgNmttqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 서버 시작\n","python -m p3ch15.request_batching_server data/p1ch2/horse2zebra_0.4.0.pth"],"metadata":{"id":"ynFQ98mLuGK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data/p1ch2/horse.jpg 업로드하여 테스트 후 결과 저장\n","curl -T data/p1ch2/horse.jpg http://localhost:8000/image --output /tmp/res.jpg"],"metadata":{"id":"ydx9F8lOuS_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이 서버는 GPU를 위해 요청을 배치로 만들고 비동기로 동작하는 등 잘 만들어졌지만 파이썬 모드를 사용하므로 메인 스레드에서 요청을 처리할 때 GIL로 인해 병렬 처리하기가 어려우며 인터넷 같은 적대적인 환경에서는 위험에 노출되기 쉬움\n","# 요청 데이터를 디코딩하는 부분은 성능이 최적화되어 있지도 않고 안전하지도 않음"],"metadata":{"id":"m79crjInukFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2절 - 모델 내보내기"],"metadata":{"id":"eCeV7SnI4S8Q"}},{"cell_type":"code","source":["# GIL이 개선된 웹서버 방해할 가능성이 있어 파이썬 인터프리터에서 파이토치 사용하는 방식이 늘 좋지는 않음\n","# 혹은 파이썬 돌리기에 성능 문제가 있거나 불가능한 임베디드 시스템 사용해야 될 수도 있음\n","# 해결책\n","# 파이토치를 들어내고 전용 프레임워크로 옮겨가기\n","# 혹은 파이썬의 파이토치 서브셋을 위한 JIT(just in time) 컴파일러 사용하며 파이토치 환경에 머무를 수 있음\n","# 파이썬 안에서 JIT 사용해도 꽤 괜찮은 최적화 해주고 GIL에서 벗어나게 도와줌\n","# 시간이 좀 걸리지만 모델을 파이토치가 제공하는 C++ 라이브러리인 libtorch나 여기서 파생된 토치 모바일(torch mobile)로 돌리는 방법"],"metadata":{"id":"FnlHF3N7u3Ce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .1 ONNX로 파이토치를 넘어서는 호환성 확보"],"metadata":{"id":"FO49gOR85gjX"}},{"cell_type":"code","source":["# 경우에 따라서 모델이 파이썬 생태계 밖에서 동작해야 할 경우 있음\n","# 임베디드 하드웨어에서 동작하기 위한 특별한 모델 배포 파이프라인 사용\n","# 이를 위해 ONNX(open neural network exchange)라는 신경망과 머신러닝 모델을 위한 상호 호환 포맷 존재\n","# 이 포맷으로 내보내면 모델은 ONNX 런타임과 같은 동일한 ONNX 호환 런타임 환경을 사용하는 모든 곳에서 실행 가능\n","# 모델이 사용하는 연산이 ONNX 표준과 타깃 런타임에서 지원하는 경우만 해당\n","# 라즈베리 파이에서 파이토치 직접 실행하는 것보다 이 방식이 더 빠른 경우 존재\n","# 일반적인 하드웨어 외에 꽤 많은 특수 AI 가속 하드웨어에서 ONNX 지원"],"metadata":{"id":"eJbokxH75ftR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 딥러닝 모델은 행렬곱, 컨볼루션, relu, tanh 같은 매우 특별한 명령어 집합으로 이뤄진 프로그램\n","# 계산을 직렬화할 수 있다면 저수준 명령어를 이해하는 다른 런타임 환경에서 실행 가능\n","# ONNX는 이런 연산과 파라미터를 기술하는 표준화된 포맷"],"metadata":{"id":"-DEMWWd77Sd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 오늘날 대부분의 딥러닝 프레임워크는 사용하는 계산을 ONNX로 직렬화할 수 있도록 지원\n","# ONNX 파일 읽어 실행도 가능(파이토치는 불가능)\n","# 엣지(edge) 디바이스라고 하는 저사양 단말 장치 중에는 ONNX 파일을 입력으로 받아 특정 디바이스를 위한 저수준 명령을 만들기도 함\n","# 최근의 클라우드 컴퓨팅 제공사들은 ONNX 파일을 업로드하면 REST 엔드포인트를 통해 서비스를 공개하게 만들기도 함"],"metadata":{"id":"tnvHHcvt79rr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델을 ONNX로 내보내려면 모델을 빈 입력으로 실행해야 함\n","# 입력 텐서의 값은 상관 없고 차원 정보와 타입만 맞으면 됨\n","# torch.onnx.export 함수를 호출해 파이토치는 모델이 수행한 계산을 추적하고 주어진 파일명으로 ONNX 파일에 직렬화해서 기록함\n","torch.onnx.export(seg_model, dummy_input, \"seg_model.onnx\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"00o3Qdv_8ib7","executionInfo":{"status":"error","timestamp":1708657982443,"user_tz":-540,"elapsed":280,"user":{"displayName":"김태윤","userId":"12351183305385272389"}},"outputId":"b11f5c7d-7cad-4bbf-f19d-ce395a57e73d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'torch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-72fe88a70259>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 입력 텐서의 값은 상관 없고 차원 정보와 타입만 맞으면 됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.onnx.export 함수를 호출해 파이토치는 모델이 수행한 계산을 추적하고 주어진 파일명으로 ONNX 파일에 직렬화해서 기록함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seg_model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["# 결과로 만들어진 ONNX 파일은 런타임에서 실행 가능하고 엣지 디바이스용으로 컴파일 될 수 있으며 클라우드 서비스에도 업로드 됨\n","# 파이썬에서 onnxruntime이나 onnxruntime-gpu를 설치 후 batch를 넘파이 배열로 얻어 사용 가능\n","# onnx_example.py (깃허브에 없음)\n","import onnxruntime\n","\n","sess = onnxruntime.InferenceSession(\"seg_model.onnx\") # ONNX 런타임 API는 모델의 정의하기 위해 세션을 사용하며 키 값 현태의 입력값들을 사용하여 run 메소드를 호출. 정적 그래프에 정의된 연산을 다루기 위한 전형적인 설정으로 생각하면 됨\n","input_name = sess.get_inputs()[0].name\n","pred_onxx, = sess.run(None, {input_name: batch})"],"metadata":{"id":"My52kQi487hA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모든 토치스크립트 연산을 표준화된 ONNX 연산으로 나타낼 수 있는 것은 암\n","# 특별한 연산을 ONNX로 내보내면 런타임에 알 수 없는 aten 연산이라는 에러 발생"],"metadata":{"id":"biaVYLQK-wfQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .2 파이토치로 내보내기: 추적"],"metadata":{"id":"xbh31Vci_BxQ"}},{"cell_type":"code","source":["# 호환성은 중요하지 않지만 파이썬 GIL은 피하고 싶거나 다른 이유로 신경망 모델을 내보내야 하는 경우, 토치스크립트 그래프라는 파이토치만의 표현 방식 사용 가능"],"metadata":{"id":"ZeRF6tVd93vP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토치스크립트 만드는 제일 단순한 방법은 추적(tracing)하는 것인데 ONNX 내보내기와 거의 비슷함\n","# ONNX 모델 내부에서 일어나는 동작과 같음\n","# torch.jit.tract 함수를 사용해 모델에 빈 입력 넣고, UNetWrapper 읽어들이고, 훈련시킨 파라미터 볼러온 후 모델 평가 모드로 만들기"],"metadata":{"id":"vGz4kIBg_Rkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 추적 전 주의할 점으로 파라미터 중 어떤 것도 기울기가 필요하면 안됨\n","# torch.no_grad() 콘텍스트 관리자는 엄격히 런타임 스위치이기 때문\n","# 모델을 no_grad 안에서 추적하도록 만들고 밖에서 실행해도 파이토치는 기울기를 기록\n","# 모델 따라 내려가고 나서 파이토치에 실행을 요청\n","# 추적된 모델은 저장된 연산 실행할 때 기울기가 필요한 파라미터가 있게 되어 기울기 필요한 상황이 유발되므로, 추적된 모델을 torch.no_grad 콘텍스트 안에서 실행"],"metadata":{"id":"zV-_GnMw_yDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.jit.trace 호출\n","# trace_example.py (깃허브에 없음)\n","import torch\n","from p2ch13.model_seg import UNetWrapper\n","seg_dict = torch.load('...')\n","seg_model = UNetWrapper(...)\n","seg_model.load_state_dict(seg_dict['model_state'])\n","seg_model.eval()\n","for p in seg_model.parameters(): # 기울기 요청하지 않도록 파라미터 설정\n","    p.requires_grad_(False)\n","\n","dummy_input = torch.randn(1, 8, 512, 512)\n","traced_seg_model = torch.jit.trace(seg_model, dummy_input) # 추적"],"metadata":{"id":"1HRduKtxAwR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추적 기능은 경고를 출력\n","# 유넷에서 크롭하기 때문에 발생하는 메시지\n","# 모델에 512*512 이미지만 넣을 것이면 문제 안됨"],"metadata":{"id":"II_6OnlxDQN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추적된 모델은 다음과 같이 저장 가능\n","torch.jit.save(traced_seg_model, 'traced_seg_model.pt')"],"metadata":{"id":"CCUCucT-DeBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이후 저장한 파일 있으면 읽어들인 후 호출 가능\n","loaded_model = torch.jit.load('traced_seg_model.pt')\n","prediction = loaded_model(batch)"],"metadata":{"id":"gNC7fZrNDnCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이토치 JIT가 모델 정장할 때 평가 모드로 동작하지 않고 파라미터도 기울기 값 사용하지 않는다는 설정 보존\n","# 저장할 때 이 부분 설정하지 않았다면 실행할 때 with torch.no_grad() 사용해야 함"],"metadata":{"id":"2Ljh_MBBEBnP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .3 추적된 모델로 만든 서버"],"metadata":{"id":"NFkQb-92EWAA"}},{"cell_type":"code","source":["# 웹서버 최종 버전 제작\n","# 추적된 사이클GAN 모델을 내보내기\n","python3 p3ch15/cyclegan.py data/p1ch2/horse2zebra_0.4.0.pth\n","-> data/p3ch15/traced_zebra_model.pt"],"metadata":{"id":"3u4EjQvnEYjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 서버에서는 get_pretrained_model 호출을 torch.jit.lead로 바꾸기만 하면 됨\n","# 이제부터 바라던 대로 모델이 GIL로부터 독립해서 동작\n","# 편의를 위해 request_batching_jit_server.py를 살짝 수정\n","# 추적된 모델 파일 경로는 명령행 인자로 받을 수 있음"],"metadata":{"id":"XfmNf6K8E16I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3절 - 파이토치 JIT 동작"],"metadata":{"id":"0b5XLq89FO_o"}},{"cell_type":"code","source":["# 파이토치 JIT는 풍부한 배포 옵션을 제공하는 것과 더불어, 파이토치에서 일어난 최근의 여러 가지 핵심적인 혁신 중 하나"],"metadata":{"id":"fU1EXUkcFOqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .1 전통적인 파이썬/파이토치를 넘어서기"],"metadata":{"id":"5G5feubiFfbi"}},{"cell_type":"code","source":["# 파이썬에서 모델 돌리지 않아 얻는 속도 면에서의 장점은 멀티스레드 환경에서만 나타나는 것 중요\n","# 중간 과정은 파이썬 객체가 아니기 때문에 파이썬 병렬화 이슈인 GIL이 연산에 영향 주지 않음"],"metadata":{"id":"JGYSKGKoFfMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 다음 연산 보기 전에 현재 연산 하나를 수행하는 식의 전통적인 파이토치 방식에서 벗어나면 연산에 대한 전체적인 관점 가질 수 있음\n","# 계산을 전체적인 관점에서 생각하는 것\n","# 이를 통해 고도화된 최적화나 높은 수준의 변환으로 가는 문 열 수 있음\n","# 보통 추론(inference) 시점에 고려하지만 훈련에서도 최적화를 통해 상당한 성능 개선을 가져오기도 함"],"metadata":{"id":"EV-OK774F5VR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예제를 통해 여러 연산 한꺼번에 고려하는 것의 이득 확인\n","# 파이토치가 GPU 상에서 일련의 연산들을 수행할 때 각각에 대해 서브 프로그램(CUDA 용어로 커널)을 호출\n","# 모든 커널은 GPU 메모리에서 입력을 얻어온 후 결과를 계산하고 저장\n","# 대부분의 시간을 메모리에 읽고 쓰는 작업에 사용\n","# 따라서 한번에 읽어 여러 연산 수행하고 마지막에 모두 쓰면 성능이 개선됨\n","# 이 아이디어는 파이토치 JIT fuser가 하는 작업 반영함"],"metadata":{"id":"SRXPx5BUGSTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5개의 입력과 2개의 출력과 7개의 중간 결과물 있는 LSTM 빌딩 블록에서 일어나는 단위 계산\n","# 모두를 하나의 CUDA 함수로 연산하고 중간 결과물을 레지스터에 두면 JIT의 메모리 읽기 횟수 12->5, 쓰기 연산 9->2, 신경망 훈련 시간 4배 감소"],"metadata":{"id":"36KwIkMrJ4lj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이썬을 벗어나기 위해 JIT를 사용한 속도 개선은 파이썬이 느리다는 소문에 비해서 대단하지 않지만, GIL을 회피하는 부분은 멀티스레드 애플리케이션에서 중요\n","# JIT 모델에서 대부분의 성능 개선은 JIT에서 제공하는 특수한 최적화 덕분이며 파이썬 오버헤드 피하는 것보다 더 정교한 최적화 과정 포함"],"metadata":{"id":"TGXtj4oyKsoy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .2 인터페이스와 백엔드 관점에서의 파이토치"],"metadata":{"id":"jlhewM98LcAB"}},{"cell_type":"code","source":["# 파이썬을 사용하지 않고 동작하는지 이해하기 위해 파이토치를 여러 부분으로 나눠 생각\n","# torch.nn 모듈은 신경마의 파라미터를 보관하며 함수형 인터페이스를 사용해 텐서 입력받고 텐서 출력\n","# 이들은 C++ 확장으로 구현되었고 C++ 수준의 자동미분이 활성화된 계층으로 넘겨짐"],"metadata":{"id":"an3Ory62La7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# C++ 함수가 이미 존재하므로 파이토치 개발자들은 이를 공식 API로 만듦\n","# 이것이 LibTorch의 핵심이며, 대응하는 파이썬의 텐서 연산을 우리가 직접 C++로 구현할 수 있게 해줌\n","# torch.nn 모듈은 파이썬에서만 사용할 수 있지만 C++ API가 이들을 각각 torch::nn 네임스페이스에 대응하여 파이썬처럼 보이지만 파이썬 언어에서 독립하여 사용할 수 있도록 설계됨"],"metadata":{"id":"vPkR4iZFL9wk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 따라서 파이썬에서 했던 작업을 동일하게 C++에서 재현 가능하지만 목표는 모델을 내보내는 것임\n","# 파이토치에는 동일한 함수 제공하는 다른 인터페이스 JIT 존재\n","# 파이토치 JIT은 연산에 대한 심볼릭 표현 제공\n","# 표현이라 함은 토치스크립트 혹은 토치스크립트IR을 의미\n","# 토치스크립트 모듈 읽고 검사하며 실행하기 위해 다룬 파이토치 API나 파이토치 JIT 함수들은 파이썬과 C++ 둘 다에서 접근 가능"],"metadata":{"id":"KzNwxJg9MXSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# C++와 파이썬 각각에서 함수 직접 호출하거나 중개자로서 JIT를 거쳐 호출\n","# 모든 경우는 결국 C++ LibTorch함수 호출하고 ATen과 연산 백엔드로 내려감"],"metadata":{"id":"FBU_c8-ONVXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .3 토치스크립트"],"metadata":{"id":"o5R-JHeFNxGY"}},{"cell_type":"code","source":["# 토치스크립트 모델 만드는 방법에는 추적에 의한 것과 스크립트에 의한 것 두 가지 방법\n","# 최상위 수준에서 두 가지 방법의 동작 방식 확인"],"metadata":{"id":"kfdCgNIINw1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추적\n","# 위에서 사용했던 추적의 경우 파이토치 모델을 샘플 입력을 사용해서 실행했음\n","# 파이토치 JIT는 허용하는 모든 함수에 대한 연산 레코딩을 위한 후크(hook) 있음\n","# JIT는 파이토치 함수가 호출돼야 동작하며 추적 중에는 모든 파이썬 코드 실행 가능하지만 JIT는 파이토치만 인지함\n","# 일반적으로 정수 튜플로 이뤄진 텐서의 차원 정보를 사용하면 JIT 동작을 확인하려 시도하지만 중간에 포기할 수 있음(유넷 추적 때 경고 떴던 이유)"],"metadata":{"id":"MlOvyWyzPLrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 스크립트\n","# 파이토치 JIT는 연산을 위한 실제 파이썬 코드를 보고 토치스크립트 IP로 컴파일 함\n","# 즉 작성한 프로그램을 JIT가 모두 캡처하는 과정은 컴파일러가 이해할 수 있는 범위로 제한된다는 의미"],"metadata":{"id":"QmgMPGSbPusJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 첫 번째 차원에 비효율적인 덧셈을 수행하는 함수로 추적과 스크립트 수행\n","import torch\n","\n","def myfn(x):\n","    y = x[0]\n","    for i in range(1, x.size(0)):\n","        y = y + x[i]\n","    return y"],"metadata":{"id":"D-VT3s45QIfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 위 함수 추적하기\n","inp = torch.randn(5,5)\n","traced_fn = torch.jit.trace(myfn, inp)\n","print(traced_fn.code)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvlWJpU3QeSQ","executionInfo":{"status":"ok","timestamp":1708663184895,"user_tz":-540,"elapsed":414,"user":{"displayName":"김태윤","userId":"12351183305385272389"}},"outputId":"295d8eca-6bbe-4827-a5ea-0447b6ad6af1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["def myfn(x: Tensor) -> Tensor:\n","  y = torch.select(x, 0, 0)\n","  y0 = torch.add(y, torch.select(x, 0, 1))\n","  y1 = torch.add(y0, torch.select(x, 0, 2))\n","  y2 = torch.add(y1, torch.select(x, 0, 3))\n","  return torch.add(y2, torch.select(x, 0, 4))\n","\n"]}]},{"cell_type":"code","source":["# 코드가 다섯 개의 행으로 인덱싱과 덧셈이 고정되어 4개 행이나 6개 행인 경우에 의도대로 동작하지 않음\n","# 이런 때 다음과 같은 스크립트 필요\n","scripted_fn = torch.jit.script(myfn)\n","print(scripted_fn.code)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zBFWj_erQr2q","executionInfo":{"status":"ok","timestamp":1708663385330,"user_tz":-540,"elapsed":278,"user":{"displayName":"김태윤","userId":"12351183305385272389"}},"outputId":"392aa745-1e6a-4e9b-83ba-b278d2310b19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["def myfn(x: Tensor) -> Tensor:\n","  y = torch.select(x, 0, 0)\n","  _0 = torch.__range_length(1, torch.size(x, 0), 1)\n","  y0 = y\n","  for _1 in range(_0):\n","    i = torch.__derive_index(_1, 1, 1)\n","    y1 = torch.add(y0, torch.select(x, 0, i))\n","    y0 = y1\n","  return y0\n","\n"]}]},{"cell_type":"code","source":["# 토치스크립트 내부 표현에 가까운 스크립트 그래프도 출력 가능\n","xprint(scripted_fn.graph)\n","# end::cell_5_code[]\n","\n","# tag::cell_5_code[]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"vEcWQuqFRikp","executionInfo":{"status":"error","timestamp":1708663551238,"user_tz":-540,"elapsed":271,"user":{"displayName":"김태윤","userId":"12351183305385272389"}},"outputId":"97facbcf-62a6-420f-81c6-ddeccf010f7e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'xprint' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-41306337fb46>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 토치스크립트 내부 표현에 가까운 스크립트 그래프도 출력 가능\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscripted_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# end::cell_5_code[]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tag::cell_5_code[]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'xprint' is not defined"]}]},{"cell_type":"code","source":["# 실제로는 torch.jit.script를 테코레이터 형태로 쓰게 되는 일이 빈번할 것임\n","@torch.jit.script\n","def myfn(x):\n","    ..."],"metadata":{"id":"-Oqp-4jYSCho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 입력값 중의하며 커스텀 trace 데코레이터를 만들어 쓸 수도 있음 (책에서 하지 않음)"],"metadata":{"id":"8pRHXvyuSmBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토치스크립트가 파이썬의 서브셋 같아 보일 수 있지만 근본적 차이 존재\n","# 파이토치는 코드 안에 타입 명세 추가 -> 정적 타이핑\n","# 프로그램의 모든 변수 값이 하나의 타입만 가지고 이 타입은 토치스크립트IP 표현으로 제한됨\n","# 프로그램 안에서 JIT는 통상 자동으로 타입을 추론하지만 스크립트 함수에서 텐서가 아닌 인자의 경우 타입은 애노테이션으로 명세해야 함"],"metadata":{"id":"NuSLE3q2VrYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이제 추적이나 스크립트 모델을 사용할 수 있음\n","# 이 모델들은 우리가 즐겨 쓰는 모듈과 거의 비슷하게 동작\n","# 추적과 스크립트 모두 샘플 입력과 함께 torch.jit.trace나 torch.jit.script로 Module 인스턴스를 전달\n","# 각각은 우리가 사용하던 forward 메소드 넘겨줌\n","# 스크립트 경우에 외부에서 호출 가능한 다른 메소드를 공개하고 싶다면 클래스 정의에서 @torch.jit.export로 데코레이션하면 됨"],"metadata":{"id":"MkwRs8dzVwOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# JIT로 처리된 모듈이 파이썬처럼 동작한다는 사실은 훈련에도 적용됨\n","# JIT 모듈을 사용하는 훈련은 일반적인 모델처럼 인터페이스를 설정할 필요가 있다는 의미"],"metadata":{"id":"XzKPfg_7Wwhs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사이클GAN, 분류 모델, 유넷 세그멘테이션과 같은 상대적으로 단순한 알고리즘 모델의 경우에는 모델을 추적할 수 있음\n","# 복잡한 모델에서 쓰기 좋은 점을 들면, 다른 스크립트로 만든 코드나 추적 코드를 스크립트로 된 함수나 추적되는 함수에서 사용할 수 있고, 모듈을 만들어 추적하거나 스크립트로 작성할 때에도  스크리브로 되었거나 추적되는 서브 모듈 사용할 수 있다는 점\n","# 또한, nn.Module 호출로 함수를 추적할 수 있는데 이때에는 기울기 요구하지 않도록 파라미터를 설정하여 추적되는 모델에 대해 파라미터가 상수로 취급되도록 만듦"],"metadata":{"id":"UfIV_dSUW9DJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .4 추적 가능하도록 토치스크립트로 만들기"],"metadata":{"id":"2izdZfdRXulQ"}},{"cell_type":"code","source":["# 자연어 처리에서 쓰이는 순환 신경망이나 탐지를 위한 빠른 R-CNN 패밀리 같은 복잡한 모델에서는 for 루프를 위한 제어 흐름을 스크립트로 작성해야 함\n","# 이때 유연성을 원했다면 추적 중 경고 메시지를 출력한 코드를 발견하게 될 것임\n","# util/unet.py\n","class UNetUpBlock(nn.Module):\n","    def center_crop(self, layer, target_size):\n","        _, _, layer_height, layer_width = layer.size()\n","        diff_y = (layer_height - target_size[0]) // 2\n","        diff_x = (layer_width - target_size[1]) // 2\n","        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])] # 추적 중 경고 메시지 출력하는 부분"],"metadata":{"id":"EP4cq7mDXt2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 여기서 JIT는 차원 정보를 가진 튜플 up.shape의 정보를 유지한 채 1차원 정수 텐서로 바꿈\n","# 이제 [2:]나 diff_x와 diff_y는 모두 추적 가능한 텐서 연산\n","# 하지만 이렇게 바뀌어도 슬라이싱은 파이썬 int가 필요한데 여기서 JIT가 끝나므로 경고 메시지 보여줌"],"metadata":{"id":"PN7JnlnYYcbp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# center_crop 스크립트 작성해서 문제 해결\n","# 호출하는 지점과 호출되는 지점 사이에서 center_crop 스크립트에 up을 전달하고 이 스크립트에서 크기를 추출함\n","# 그러고 @torch.jit.script 데코레이터 추가하면 됨\n","# 코드는 깃허브에 없음"],"metadata":{"id":"bKjAFQ-fZKSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 스크립트화할 수 없는 것들 모두 C++로 구현한 커스텀 연산으로 옮겨넣는 등의 방법도 있음\n","# 토치비전 라이브러리는 마스크 R-CNN 모델 내부 특수 연산에 대해 커스텀 연산으로 옮겨 넣는 방법 사용"],"metadata":{"id":"Zi7i8XUhZ7Vq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4절 - LibTorch: C++ 파이토치"],"metadata":{"id":"Y5OQHettaIpg"}},{"cell_type":"code","source":["# C++로 직접 모델 내보내는 법"],"metadata":{"id":"Zey85B1oaIM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 말-얼룩말 사이클GAN JIT 모델을 C++ 프로그램으로 돌리기"],"metadata":{"id":"UYc6KbZsjWz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .1 JIT로 처리된 모델을 C++로 실행하기"],"metadata":{"id":"boh3gztljhSR"}},{"cell_type":"code","source":["# CImg 라이브러리 사용"],"metadata":{"id":"F_aFuyQljgZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# JIT로 처리된 모델 실행\n","# cyclegan_jit.cpp\n","#include \"torch/script.h\" # 파이토치 스크립트 헤더와 네이티브 JPEG 지원 CImge 인클루드\n","#define cimg_use_jpeg\n","#include \"CImg.h\"\n","using namespace cimg_library;\n","int main(int argc, char **argv) {\n","// end::part1[]\n","  if (argc != 4) {\n","    std::cerr << \"Call as \" << argv[0] << \" model.pt input.jpg output.jpg\"\n","              << std::endl;\n","    return 1;\n","  }\n","  CImg<float> image(argv[2]); # 이미지 읽어 디코딩하여 float 배열에 삽입\n","  # 입력으로부터 여기에서 출력 텐서 생성해야 함\n","  CImg<float> out_img(output.data_ptr<float>(), output.size(2), # data_prt<float>()메소드는 텐서 저장소에 해당하는 포인터 반환. 이 포인터와 차원정보를 사용해 출력 이미지 생성\n","                      output.size(3), 1, output.size(1));\n","  out_img.save(argv[3]); # 이미지 저장\n","  return 0;\n","}"],"metadata":{"id":"FHD7fy2nkNqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이토치를 위해 C++ 헤더 torch/script.h 인클루드\n","# 이후 CImge 라이브러리 인클루드하고 설정\n","# main 함수에서 명령행 인자로 주어진 파일의 이미지 읽고 CImge로 리사이즈해서 227*227 이미지 가지는 CImge<float> 변수 image 얻음\n","# 프로그램 끝에는 (1, 3, 277, 277) 텐서에서 동일한 타입의 out_img 만들어 저장"],"metadata":{"id":"I-8KZerwlBlq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지에서 입력 텐서 만들고 모델 로딩해 입력 텐서로 실행하는 실제 연산 부분\n","#cyclegan_jit.cpp\n","auto input_ = torch::tensor(\n","  torch::ArrayRef<float>(image.data(), image.size())); # 이미지 데이터 텐서에 입력\n","  auto input = input_.reshape({1, 3, image.height(),\n","\t\t\t       image.width()}).div_(255); # 리세이프 후 크기 조정을 해 CImge 형태에서 파이토치 형으로 변경\n","\n","  auto module = torch::jit::load(argv[1]); # JIT로 처리된 모델이나 함수를 파일에서 읽어들이기\n","\n","  std::vector<torch::jit::IValue> inputs; # 입력을 원소 하나인 IValue 벡터로 패킹\n","  inputs.push_back(input);\n","  auto output_ = module.forward(inputs).toTensor(); # 모듈을 호출하고 결과 텐서를 추출. 효율성 높이기 위해 소유권이 이동되었기에, IValue 유지하면 나중에 비어 있게 됨\n","\n","  auto output = output_.contiguous().mul_(255); # 결과를 메모리에서 연속적인 형태로 만듦"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"Mm6NSLYald2C","executionInfo":{"status":"error","timestamp":1708668891545,"user_tz":-540,"elapsed":270,"user":{"displayName":"김태윤","userId":"12351183305385272389"}},"outputId":"06799904-6652-4a04-a30f-90c845122eaa"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-46-555e16bac6b9>, line 3)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-555e16bac6b9>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    auto input_ = torch::tensor(\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["# 파이토치가 특정 순서로 다량의 메모리 공간에서 값을 기록하고 있음\n","# CImge도 마찬가지로 image.data()로 이 메모리에 대한 float 배열 포인터를 얻을 수 있고 image.size()로 요소 개수를 얻을 수 있음\n","# 이 둘로 더 똑똑한 레퍼런스인 torch:ArrayRef를 만들 수 있고 이것으로 torch::tensor 생성자를 사용해 리스트처럼 파싱할 수 있음"],"metadata":{"id":"AkTabvcvmi7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 현재 텐서는 1차원이므로 리세이프 해야 함\n","# CImge는 파이토치와 같은 순서(채널, 행, 열) 사용\n","# CImge는 0...255 범위 사용하고 우리 모델은 0...1 사용하므로 나눴다가 나중에 다시 곱할 것임"],"metadata":{"id":"7sC-6algm-6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추적할 모델 읽는 것은 torch::jit::load를 사용하면 간단하게 할 수 있음\n","# 파이썬과 C++ 사이 연결하는 역할 하는 파이토치 추상화에 대해 다루기\n","# 입력은 모든 값에 대한 제네릭 타입인 IValue로 래핑해야 함\n","# JIT 내부 함수로 IValue의 벡터가 전달되므로 이렇게 선언하고 입력 텐서에 push_back 해야 함\n","# 이러면 텐서를 자동적으로 IValue로 래핑하게 됨\n","# 순방향으로 IValue의 벡터를 넣어주면 한 개의 IValue를 돌려받음\n","# 결과 IValue에서 .toTensor를 사용해 다시 텐서를 얻을 수 있음"],"metadata":{"id":"qMme8RU7ngqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이를 통해 IValue에는 타입이 있다는 점(여기서는 Tensor)과 복수의 int64_t나 double, 텐서 리스트 등을 가질 수 있다는 점 확인 가능\n","# 여러 출력이 있는 경우 텐서 리스트 가지는 IValue 얻는데, 파이썬 호출 컨벤션으로 만들어졌을 것임\n","# .toTensor로 IValue에 있는 텐서 꺼내면 IValue는 소유권 변경해줌(즉 무효화)\n","# 경우에 따라 모델이 인접한 데이터 반환하지 않을 수 있음\n","# CImge에는 연속적인 블럭 제공해야 하므로 contiguous 호출\n","# 주어진 메모리 안에서 작업할 때 연속적인 텐서를 스코프 내의 변수로 할당하는 부분은 중요\n","# 파이썬처럼 파이토치는 더 이상 사용하지 않는 텐서의 메모리는 해제함"],"metadata":{"id":"2_j-XAz-oj3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이토치 링킹하는 작업은 복잡하므로 견본 CMake 파일 사용\n","# CMakeLists.txt\n","cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\n","project(cyclegan-jit) # 프로젝트 이름\n","\n","find_package(Torch REQUIRED) # 토치를 필요한 패키지로 설정\n","set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\n","\n","add_executable(cyclegan-jit cyclegan_jit.cpp) # cyclegan_jit.cpp 소스 파일 컴파일 후 cyclegan-jit 실행파일 생성\n","target_link_libraries(cyclegan-jit pthread jpeg X11) # CImg에 필요한 라이브러리 링킹. CImg 자체는 인클루드만으로 사용 가능하기에 등장하지 않음\n","target_link_libraries(cyclegan-jit \"${TORCH_LIBRARIES}\")\n","set_property(TARGET cyclegan-jit PROPERTY CXX_STANDARD 14)"],"metadata":{"id":"Cic6WGooqdwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 소스 코드 디렉토리 아래 빌드 디렉토리 만드는 방법이 제일 좋음\n","# CMake를 실행 후 마지막으로 make 실행\n","# 빌드 끝나면 cyclegan-git 프로그램 만들어지며 실행 가능"],"metadata":{"id":"Ojy2NrHYs-MJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이썬 없이 파이토치 모델 실행 완료\n","# 애플리케이션 출시를 원하면 라이브러리를 실행 파일이 있는 곳으로 복사해서 항상 찾을 수 있게 만들어야 함"],"metadata":{"id":"wrccCx_AtRuc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### .2 시작부터 C++: C++ API"],"metadata":{"id":"QsvVQ4IKtdwa"}},{"cell_type":"code","source":["# C++ 모듈러 API는 파이썬처럼 보이도록 만들어짐\n","# 사이클GAN 생성기를 JIT 없이 C++만으로 정의된 버전으로 변경\n","# 사전 훈련된 가중치는 필요하므로 모델을 추적한 버전을 저장"],"metadata":{"id":"WyTITFM8tcIo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 코드 관리에 필요한 세부 내용인 인클루드와 네임스페이스부터 시작\n","#include <torch/torch.h>  # torch/torch.h 헤더와 CImg 인클루드\n","#define cimg_use_jpeg\n","#include <CImg.h>\n","using torch::Tensor;  # torch::Tensor를 main 네임스페이스에 추가"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"nqqQVN1Mtv1b","executionInfo":{"status":"error","timestamp":1708670863362,"user_tz":-540,"elapsed":293,"user":{"displayName":"김태윤","userId":"12351183305385272389"}},"outputId":"a794048d-bb44-4b95-eca8-59d12f200554"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-54-8d13c1ad7632>, line 5)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-8d13c1ad7632>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    using torch::Tensor;  # torch::Tensor를 main 네임스페이스에 추가\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["# ConvTransposed2d가 임의로 정의된 것 확인 가능\n","# C++ 모듈러 API는 개발 중이고 파이토치 1.4에서 만들어진 ConvTransposed2d 모듈은 두 번째 인자로 옵션값 받기 때문에 Sequential에서 사용 불가\n","# 2장의 파이썬 CycleGAN 생성기와 같은 구조 가지게 만들 것임"],"metadata":{"id":"1qiQApxOuEVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 잔차 블럭 확인\n","# cyclegan_cpp_api.cpp\n","struct ResNetBlock : torch::nn::Module {\n","  torch::nn::Sequential conv_block;\n","  ResNetBlock(int64_t dim)\n","      : conv_block(  # Sequential 초기화 후 서브모듈 추가\n","            torch::nn::ReflectionPad2d(1),\n","            torch::nn::Conv2d(torch::nn::Conv2dOptions(dim, dim, 3)),\n","            torch::nn::InstanceNorm2d(\n","\t       torch::nn::InstanceNorm2dOptions(dim)),\n","            torch::nn::ReLU(/*inplace=*/true),\n","\t    torch::nn::ReflectionPad2d(1),\n","            torch::nn::Conv2d(torch::nn::Conv2dOptions(dim, dim, 3)),\n","            torch::nn::InstanceNorm2d(\n","\t       torch::nn::InstanceNorm2dOptions(dim))) {\n","    register_module(\"conv_block\", conv_block); # 할당할 모듈 등록하지 않으면 문제 발생\n","  }\n","\n","  Tensor forward(const Tensor &inp) {\n","    return inp + conv_block->forward(inp); # 예상했던 대로 순방향 전파는 단순함\n","  }\n","};"],"metadata":{"id":"zrHv-jrXubT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이썬에서처럼 torch::nn::Module의 서브 클래스 등록\n","# 잔차 블럭은 연속된 conv_block 서브 모듈 가짐"],"metadata":{"id":"5OavCfi7u2zp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 마찬가지로 파이썬에서처럼 서브 모듈을 초기화해야 함\n","# 이를 위해 C++ 초기화 문 사용\n","# 파이썬의 __init__ 생성자에서 서브 모듈을 구성했던 것과 유사\n","# 파이썬에서 멤버로 할당하고 등록하기 위해 __setattr__ 리다이렉트를 지원하기 위한 인트로스펙션이나 후킹 같은 기능 C++에서 제공 안 함"],"metadata":{"id":"qNmG8OU7wXyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 키워드 인자가 없기 때문에 기본 인자로 파라미터를 정의하면 어색하므로 모듈은 통상 options 인자 받음\n","# 파이썬 옵션 키워드 인자는 옵션 객체의 메소드에 대응하고 여러 개를 이어서 작성 가능"],"metadata":{"id":"Ctlp-q7Qwv7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 멤버 등록과 할당은 항상 동기화되도록 주의해야 함\n","# 하지 않으면 예상치 못한 일 발생"],"metadata":{"id":"z40GsvOYxAI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이썬과 대조적으로 모듈에 대해 m->forward(...)를 호출할 필요도 있음\n","# 어떤 모듈은 직접 가능하지만 Sequential은 안됨"],"metadata":{"id":"4Ybrkx94xJId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 호출 규약에 대해, 함수에 제공한 텐서를 어떻게 수정하느냐에 따라 텐서에 대한 텐서 인자를 수정하지 않은 경우에는 const Tensor&로, 수정한 경우에는 Tensor로 항상 전달해야 함\n","# 텐서는 Tensor로 반환해야 함\n","# (Tensor&)처럼 상수가 아닌 레퍼런스 형태의 인자 타입을 반환하면 컴파일러 과정 중 파싱 에러 유발함"],"metadata":{"id":"iRYDSIZjxTSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 주 생성기 클래스에서는 클래스 이름을 전형적인 C++ API 형태로 사용하고, TORCH_MODULE 매크로 사용해 토치 모듈인 ResNetGenerator로 승격할 것임\n","# 모듈을 되도록 레퍼런스나 공유 포인터로 다루기 위해, 래핑하여 클래스로 만듦\n","# cyclegan_cpp_api.cpp\n","struct ResNetGeneratorImpl : torch::nn::Module {\n","  torch::nn::Sequential model;\n","  ResNetGeneratorImpl(int64_t input_nc = 3, int64_t output_nc = 3,\n","                      int64_t ngf = 64, int64_t n_blocks = 9) {\n","    TORCH_CHECK(n_blocks >= 0);\n","    model->push_back(torch::nn::ReflectionPad2d(3)); # 생성자에서 Sequential 컨테이너에 모듈 추가. 이를 통해 for 루프 내 모듈에 대한 변수값 추가\n","    ...\n","      model->push_back(torch::nn::Conv2d(\n","          torch::nn::Conv2dOptions(ngf * mult, ngf * mult * 2, 3)\n","              .stride(2)\n","              .padding(1))); # 옵션 사용\n","    ...\n","    register_module(\"model\", model);\n","  }\n","  Tensor forward(const Tensor &inp) { return model->forward(inp); }\n","};\n","\n","TORCH_MODULE(ResNetGenerator); # ResNetGenerator로 ResNetGeneratorImpl를 래핑. 이름이 일치하도록 만드는 것은 중요"],"metadata":{"id":"g5vihGbexq9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이썬 ResNetGenerator 모델에 대한 완벽한 C++ 대응 구현 완료\n","# main 함수에서 파라미터 로딩하고 모델 실행\n","# 이미지를 CImg로 읽고 텐서로 변환하며 텐서를 이미지로 바꾸는 작업은 앞의 작업과 동일\n","# 추가로 이미지 저장 대신 출력\n","# cyclegan_cpp_api.cpp\n","ResNetGenerator model; # 모델 인스턴스화\n","...\n","torch::load(model, argv[1]); # 파라미터 로딩\n","...\n","cimg_library::CImg<float> image(argv[2]);\n","image.resize(400, 400);\n","auto input_ =\n","    torch::tensor(torch::ArrayRef<float>(image.data(), image.size()));\n","auto input = input_.reshape({1, 3, image.height(), image.width()});\n","torch::NoGradGuard no_grad;          # 가드 변수 선언. 기울기 사용 끄는 기간 명시하기 위해 {...} 블럭에 넣는 것도 방법\n","\n","model->eval();                       # 파이썬처럼 eval 모드 키기\n","\n","auto output = model->forward(input); # 모델이 아닌 forward 호출\n","...\n","cimg_library::CImg<float> out_img(output.data_ptr<float>(),\n","\t\t\t\t    output.size(3), output.size(2),\n","\t\t\t\t    1, output.size(1));\n","cimg_library::CImgDisplay disp(out_img, \"See a C++ API zebra!\"); # 이미지를 출력한 후 키 입력 기다릴 필요 있음\n","while (!disp.is_closed()) {\n","  disp.wait();\n","}"],"metadata":{"id":"TCF0Cf2LyunR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 흥미로운 변경 사항은 모델을 어떻게 만들고 실행하는지\n","# 모델 타입으로 변수 선언해 모델을 인스턴스화 함\n","# torch::load를 통해 모델 로딩(모델 래핑한 점 중요)\n","# JIT가 저장한 파일로 동작하는 점이 다름"],"metadata":{"id":"j5Ac6MAkzu7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 실행 시 with torch.no_grad(): 구문에 해당하는 코드 필요\n","# 이럴 때는 NoGradGuard 타입 변수 인스턴스화하고 기울기 필요 없는 동안 스코프 안에 살려두면 됨\n","# 파이썬처럼 model->eval() 호출해 평가 모드로 설정 가능\n","# 입력 텐서로 model->forward 호출하고 결과도 텐서로 받으며 JIT 개입하지 않으므로 IValue 패킹과 언패킹 필요 없음"],"metadata":{"id":"CLaIHbccz_o8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5절 - 모바일"],"metadata":{"id":"B7MaMWgO0dKj"}},{"cell_type":"code","source":["# 모바일 기기로 모델 배포하는 경우\n","# 안드로이드로 한정"],"metadata":{"id":"qSa_ABiz0Xt0","executionInfo":{"status":"ok","timestamp":1708763321420,"user_tz":-540,"elapsed":3,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# 파이토치 C++ 부분인 LibTorch는 안드로이드용으로 컴파일 가능\n","# 안드로이드 JNI(andriod java native interface)를 사용해 자바로 만든 앱에서 접근 가능\n","# JIT로 처리된 모델 로딩하고, 입력을 텐서와 IValue로 변환하고, 이를 사용해 모델 실행하고 결과 받는데 필요한 파이토치 함수 필요\n","# 파이토치 모바일이라 부르는 작은 라이브러리로 래핑되어 있음"],"metadata":{"id":"YZlq-Eq-Oupp","executionInfo":{"status":"ok","timestamp":1708763460402,"user_tz":-540,"elapsed":3,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 안드로이드에서 앱 만드는 일반적인 방법은 스튜디오 IDE 사용하는 것\n","# 안드로이드 스튜디어 템플릿(빈 자바 앱) 중 하나를 사진 찍는 앱으로 변환해서 얼룩말 사이클GAN 모델을 돌리고 결과를 출력할 것임\n","# 안드로이드 예제 앱 활용"],"metadata":{"id":"dkA_FEAKO2Ni","executionInfo":{"status":"ok","timestamp":1708763544986,"user_tz":-540,"elapsed":3,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 탬플릿 사용하기 위해 필요한 세 가지\n","# 1. UI 정의\n","# 사진 찍고 이미지 변환하는 headline이라는 TextView와 사진을 보여주는 ImageView인 image_view 사용\n","# 사진 찍는 기능은 카메라 앱에 의존"],"metadata":{"id":"DUfAuicyRrOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. 파이토치를 의존성으로 정의: 앱의 build.gradle 파일 편집해 pytorch_andriod와 pytorch_andriod_torchvision 삽입\n","# build.gradle에 추가할 부분\n","dependencies {\n","    ...\n","    implementation 'org.pytorch:pytorch_android:1.3.0'\n","    implementation 'org.pytorch:pytorch_android_torchvision:1.3.0'\n","}"],"metadata":{"id":"JRGFr6tKPnrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추적한 모델은 assets으로 넣기"],"metadata":{"id":"FQrdtKJdRuXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. Activity에서 파생된, 메인 코드를 포함하는 우리의 자바 클래스를 일부 발췌해 살펴보기\n","# MainActivity.java\n","...\n","import org.pytorch.IValue;\n","import org.pytorch.Module;\n","import org.pytorch.Tensor;\n","import org.pytorch.torchvision.TensorImageUtils;\n","...\n","public class MainActivity extends AppCompatActivity {\n","    private org.pytorch.Module model; # JIT로 처리된 모델을 담고 있음\n","\n","    @Override\n","    protected void onCreate(Bundle savedInstanceState) {\n","        ...\n","        try {\n","            model = Module.load(assetFilePath(this, \"traced_zebra_model.pt\")); # 파일에서 모듈 로딩\n","        } catch (IOException e) {\n","            Log.e(\"Zebraify\", \"Error reading assets\", e);\n","            finish();\n","        }\n","        ...\n","    }\n","    ...\n","}"],"metadata":{"id":"rS8oOlRgQp31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# org.pytorch 네임스페이스에서 임포트할 것이 있음\n","# 자바이기 때문에 전형적으로 IValue, Module, Tensor 임포트하며, 텐서와 이미지 간의 변한을위해 org.pytorch.torchvision.TensorImageUtils도 사용"],"metadata":{"id":"_eqRsLbKRcsW","executionInfo":{"status":"ok","timestamp":1708764166710,"user_tz":-540,"elapsed":268,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 먼저 모델 담아놓을 변수 선언\n","# 그리고 액티비티의 onCreate에서 앱 시작되면 Model.load 모세도 사용해 인자로 주어진 위치에서 모듈 읽어들이기\n","# assets으로 주어지는 앱 데이터는 파일 시스템에서 쉽게 접근하기 어려움\n","# 따라서 assetFilePath 유틸리티 메소드로 에셋을 파일 시스템 위치에 복사\n","# 마지막으로 자바에서는 코드가 던지는 모든 예외 잡아줘야 함"],"metadata":{"id":"OmgkVX1nR_bh","executionInfo":{"status":"ok","timestamp":1708764284784,"user_tz":-540,"elapsed":5,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# 안드로이드의 Intent 메커니즘을 사용해 카메라 앱에서 이미지 얻으면 모델에 넣고 돌린 후 사진 표시\n","# 이 작업은 onActivityResult 이벤트 핸들러가 수행\n","# MainActivity.java\n","@Override\n","protected void onActivityResult(int requestCode, int resultCode, Intent data) {\n","    if (requestCode == REQUEST_IMAGE_CAPTURE && resultCode == RESULT_OK) { # 카메라 앱이 사진 찍을 때 실행\n","        // this gets called when the camera app got a picture\n","        Bitmap bitmap = (Bitmap) data.getExtras().get(\"data\");\n","\n","        final float[] means = {0.0f, 0.0f, 0.0f}; # 정규화 진행 부분. 기본 이미지가 0...1 범위의 값을 가지므로 변경할 필요가 없음. 따라서 시프트는 0, 스케일은 1로 지정\n","        final float[] stds = {1.0f, 1.0f, 1.0f};\n","        // preparing input tensor\n","        final Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap, # 비트맵으로 텐서 제작. 토치비전의 ToTensor와 정규화 합친 단계\n","                means, stds);\n","\n","        // running the model\n","        final Tensor outputTensor = model.forward(IValue.from(inputTensor)).toTensor(); # C++과 유사한 부분\n","        Bitmap output_bitmap = tensorToBitmap(outputTensor, means, stds, Bitmap.Config.RGB_565); # tensorToBitmap은 직접 만든 것임\n","\n","        ImageView image_view = (ImageView) findViewById(R.id.imageView);\n","        image_view.setImageBitmap(output_bitmap);\n","    }\n","}"],"metadata":{"id":"2OBj5oohScOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 안드로이드에서 얻은 비트맵을 텐서로 변환하는 작업은 TensorImageUtils.bitmapToFloat32Tensor 함수가 수행\n","# 두 개의 float 배열인 means와 stds를 bitmap과 함께 지정\n","# 입력 데이터셋의 평균과 표준편차를 지정해 토치비전의 Normalize 변환처럼 평균 0과 단위 표준편차 가지도록 함\n","# 안드로이드에서 이미 0..1 범위의 이미지 전송하므로 모델에 바로 넣을 수 있기 때문에 평균은 0, 표준편차는 1로 설정해서 정규화 과정으로 이미지 손상 없게 할 것"],"metadata":{"id":"Jzehq-bASwOV","executionInfo":{"status":"ok","timestamp":1708764730182,"user_tz":-540,"elapsed":289,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# model.forward 호출 부근에서 C++ JIT에서와 같이 IValue 래핑과 언래핑 수행\n","# 벡터 대신 하나의 IValue를 forward가 입력받는 것이 다름\n","# 마지막으로 비트맵이 필요한데 파이토치가 이 작업을 하지 않으므로 tensorToBitmap 작성"],"metadata":{"id":"jfkjhMXMUI9_","executionInfo":{"status":"ok","timestamp":1708764829435,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 사진 요청하는 코드까지 추가해 만든 지브라파이 앱 화면을 볼 수 있음\n","# 안드로이드에서 파이토치의 모든 연산에 대한 완전한 버전을 만듦\n","# 이로 인해 작업에서 사용하지 않는 연산까지 다 포함했으므로 공간 절약을 위해 불필요한 연산 제거할 필요가 있을 거싱ㅁ\n","# 파이토치 1.4에서는 필요한 연산만을 포함하는 커스텀 파이토치 라이브러리 빌드 가능"],"metadata":{"id":"3I2xQ05oUhSe","executionInfo":{"status":"ok","timestamp":1708764924525,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#### .1 효율성 개선: 모델 설계와 양자화"],"metadata":{"id":"CzjyZKTsU41U"}},{"cell_type":"code","source":["# 모바일에서의 모델 동작 연구하다 보면 모델을 더 빨리 동작시키는 것을 생각하게 됨\n","# 메모리 사용 줄이고 모델 연산을 추적해 내려가면 가장 처음 눈에 띄는 것은 모델 자체 효율화하는 것\n","# 입출력 사이의 매핑을 동일하지만 더 적은 파라미터와 연산으로 계산하는 방법 -> 증류(distillation)\n","# 각 가중치 중에 작거나 부적절한 경우 제거하는 경우도 증류에 해당\n","# 다른 경우로는 신경망의 여러 층을 하나로 합치거나 큰 모델의 출력을 완전히 다르고 단순한 모델로 재현하도록 훈련 시키는 경우도 있음"],"metadata":{"id":"t2HOTWeBU4hP","executionInfo":{"status":"ok","timestamp":1708765109189,"user_tz":-540,"elapsed":3,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 다른 접근 방법으로 각 파라미터와 연산의 크기를 줄이는 것\n","# 파라미터로 32비트 부동소수점 사용하는 대신 정수로 동작하도록 변환하는 것 -> 양자화(quantization)"],"metadata":{"id":"uMb4xSseVLdp","executionInfo":{"status":"ok","timestamp":1708765153301,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# 파이토치는 양자화 텐서를 제공\n","# torch.float과 torch.double 혹은 torch.long와 비슷한 스칼라 타입으로 제공\n","# 가장 보편적인 양자화 텐서는 torch.qunit8과 torch.qint8\n","# 파이토치는 디스패치 메커니즘 사용하기 위해 어런 식의 독립된 스칼라 타입 사용함"],"metadata":{"id":"YHZyDYnlVwRd","executionInfo":{"status":"ok","timestamp":1708765292690,"user_tz":-540,"elapsed":281,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# float 대신 정수 사용하면 성능 좋아지지만 약간의 품질 저하 발생\n","# 품질 저하가 크지 않은 원인으로 두 가지 이유\n","# 반올림 오차는 근본적으로 무작위로 고려하고 컨볼루션과 선형 계층을 가중 평균으로 간주하면 반올림으로 인한 오차는 어느 정도 상쇄된다고 볼 수 있음\n","# 이 방식은 float32의 20비트 이상인 상대적인 정밀도를 int가 제공하는 7비트로 줄여줌\n","# 이 밖에 양자화는 부동소수점을 고정된 정밀도로 바꿔주는 효과 있음\n","# 즉 제일 큰 값은 7비트 정밀도로 해석되며 제일 큰 값의 1/8 정도의 값은 7-3=4비트를 가짐\n","# 하지만 L1 정규화가 일어나면 양자화로 인해 작은 값에 대해 적은 정밀도를 가지는 부분을 감당할 수 있을 것으로 생각하며 실제로 많은 경우에 그럼"],"metadata":{"id":"Be67QEh1WGO-","executionInfo":{"status":"ok","timestamp":1708765519105,"user_tz":-540,"elapsed":340,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## 6절 - 최근 기술: 파이토치 모델 엔터프라이즈 서빙"],"metadata":{"id":"MA4-itYmXMi0"}},{"cell_type":"markdown","source":["## 7절 - 결론"],"metadata":{"id":"6bidmVIKXeUM"}},{"cell_type":"code","source":["# 이것으로 모델을 만들어 원하는 곳에 적용하는 방법에 대한 여행 마침\n","# 제품화된 토치 서빙은 아직 사용하기에는 이른 감이 있지만 잘 만들어지면 JIT로 모델을 내보내기 할 수 있을 거임\n","# 이제 우리는 모델을 네트워크 서비스나 C++ 애플리케이션, 모바일 등으로 배포하는 법 알게 됨"],"metadata":{"id":"ZI_bF9DcXJkA","executionInfo":{"status":"ok","timestamp":1708765706544,"user_tz":-540,"elapsed":3,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## 8절 - 연습 문제"],"metadata":{"id":"5getD_4IX32U"}},{"cell_type":"code","source":["# 1. 흥미 유발하는 프로젝트 하나 골라보라. 캐글을 추천한다."],"metadata":{"id":"lLmFZPLuX3av","executionInfo":{"status":"ok","timestamp":1708765738040,"user_tz":-540,"elapsed":299,"user":{"displayName":"김태윤","userId":"12351183305385272389"}}},"execution_count":17,"outputs":[]}]}