5
신경망의 특징

6
~~~~
일반적인 경우 x, o는 단순 스칼라값 혹은 여러 스칼라값을 가진 벡터
w는 단일 스칼라 혹은 행렬, b는 스칼라나 벡터
만약 벡터인 경우에는 여러 개의 뉴런을 의미하므로 뉴런 계층이라고 부름

7
활성 함수로 인하여 오차 곡선의 모양이 이전 선형 모델과 차이를 보임
이전 선형 모델은 오차 곡선이 볼록 함수 형태여서 최솟값이 하나로 정의됨
즉 파라미터를 조정하여 정답에 가까워지도록 추정하는 의미를 지님
신경망은 오차 곡선의 모양이 다름 
정해진 답이 없이 유용한 출력을 만드는 파라미터 획득을 목표로 함
즉 참값에 근사한다는 의미를 지님

8
~~~

9
대표적인 활성 함수를 보여주는 그림임
ReLU가 가장 성능이 좋아 최신(2020년 기준) 연구 결과에 자주 쓰임
시그모이드는 초반에만 쓰이다가 잘 안 쓴다는 점
제한이 따로 없으므로 좋은 결과 나오는 함수 사용

10
신경망 모델은 고차원의 비선형 작업에도 근사할 수 있어 파라미터를 잘 추정할 수 있음
데이터를 표현하는 함수에 대해 고민할 필요가 없음

11
그 이유는 그림에 나와 있음
책의 그림을 구현한 것인데 두 개의 계층만으로 다양한 모양을 만들 수 있음

13
따라서 고수준의 비선형적 현상에 대해 명확한 모델 없이 구현이 가능함
입출력쌍과 손실 함수를 제공해 일반 모델을 특정 작업에 최적화 시킬 수 있음
명시적인 모델은 문제 해결에 한계가 있으므로 데이터에 기반해 해결해야 함
이런 부분에서 모델이 학습한다는 것은 큰 의미를 지님

15
~~~
Module의 코드 중 일부

16
nn.Linear 생성자 확인
입력 크기와 출력 크기와 편향값 포함 여부를 인자를 받는 것 확인

17
nn.Linear가 가중치와 편향값 가지는 것 확인

18
(10, 1)크기의 ones텐서 입력하여 nn.Linear실행하는 모습 확인
전 페이지의 가중치와 편향값 보면서 w*x+b인 것 확인
(B = 배치의 크기, Nin = 입력 피처의 크기일 때, 크기가 B*Nin인 입력 텐서로 모델 실행)

19
배치를 수행하는 이유는
연산량을 크게 만들어 병렬 연산에 최적화된 GPU의 자원을 최대한 활용하기 위함

20
그림과 같이 모델을 설정해서 학습을 시켰을 때 가중치와 편향값 파라미터 확인 가능

21
훈련 루프는 확인

22
손실 함수를 nn.Module에 있는 nn.MSELoss로 변경하여 학습

24
nn.Sequential을 이용해 신경망을 구현하는 모습

25
신경망의 파라미터를 확인하는 모습

26
orderedDict는 그냥 Dict형으로 보면 됨
그림은 각각의 층에 이름을 붙인 모습
계층의 이름을 이용해 원하는 파라미터에 접근하는 모습

27
신경망 학습에서 사용하는 모습

28
선형으로 주어진 문제를 학습하여 나온 결과를 그래프로 그림
O는 입력 데이터 X는 출력 데이터

30
문제 해결을 위한 코드 그림
실제로 바꾸는 부분은 seq_model 부분과 learning_rate(오른쪽 그림 윗 부분)

31
예상과는 다르게 입출력 피처수(은닉층 뉴런의 수)가 클 수록 선형이 되었음
완전히 과적합하는 모델은 만들기 힘들 것으로 보임
최대한 근접하게 한 것이 다음 그래프

33
다음 문제 와인데이터 학습한 모델 만들기를 위한 코드
데이터 전처리하는 부분과 첫 계층 입력 피처가 늘어난 것 이외에는 같음
(t_u.shape = torch.Size([4898, 1, 11]), t_c.shape = torch.Size([4898, 1]))

34
문제에 대한 답